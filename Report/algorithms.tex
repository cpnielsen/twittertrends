\section{Streaming algorithms}\label{related work}
\subsection{Introduction}\label{algo-intro}
\begin{quote}\textit{
Twitter and other social networks are structured to accomodate personal communication across large networks of friends, and a such produce enormous amounts of data. The open availability of this data through developer APIs makes it's an interesting source for useful real-time information extraction using streaming algorithms \cite{genderprediction}. 
}
\end{quote}

This section briefly describes our goals for the project and how we, both theoretically and practically, make use of streaming algorithms to achieve these goals. We go through the process of finding suitable variables for our algorithms and using appropriate data structures.

\subsection{Goal}\label{algo-goals}
Our goal is to find the trending topics on twitter over a given time period. The idea is that our algorithm will be optimized for different intervals, 1-hour and 24-hour periods, and that at any given time it can be queried to get the actual topics trending. Additionally, we wish to find, for these topics, how many distinct tweets are posted to see if popular topics are the result of aggressive re-tweeting or actual interest in the topic as a whole.

To achieve this, we need to make use of streaming, lossy algorithms, for two major reasons:
\begin{enumerate}
    \item \textbf{To minimize our memory usage and allow for shorter processing time}
        It would be impractical, if not impossible, to keep the data for all tweets in-memory for a 24-hour period. In addition, processing 500.000 tweets in one go would take a significant amount of time - with streaming algorithms we can do simple calculations per tweet and spread the load.

    \item \textbf{To keep a running total of the results that we can query at any time}
        With an algorithm that works on a known set of data, we need to first collect that set of data and then post-process it. Additionally, we need to assign chunks of data to be processed at known intervals. Streaming algorithms allow us to get results at any time we desire, although the program have to have been running for the specified interval to get accurate results, based on our selection of constants.
\end{enumerate}

\input{frequentitems.tex}

\subsection{Finding distinct values}\label{algo-distinct}
K-minimum values (KMV) is is a probalistic distinct value counting algorithm, that is intuitive and easy to implement \cite{kmv}. Suppose we have a good hash function that return evenly distributed values in the hash space $[0-1]$, then you could estimate the number of distinct values you have seen by knowing the average spacing between values in the hash space. The main challenge is to find a good hash function, and to select the number of minimum $k$ values on which to approximate the average spacing. If the hash values were indeed evenly distributed, we could keep just keep track of the minimum value, a get a good estimate of distinct values. However taking only one value opens up to a lot of variance and would rely heavily upon the``goodness'' of the hash function. In order to improve this Bar-Yossef\cite{Bar-Yossef} suggests keeping the k-smallest values, to give a more realistic estimate.

\subsubsection{Choice of hash function}\label{hash-function}
As mentioned in section \ref{algo-distinct} it's important to choose a hash function with an evenly dsitributed output to get a good estimate of distinct items with KMV. We are looking for a function that performs well for hashing text of lengths of approx. 3 to 140 characters. 

Cpesyna \cite{KMV1} performed an experiment with 4 different hash functions on different kinds of natural language inputs and varying sizes of $k$, among the input types were 99.171 english words, which we assume that most of our data is aswell. It was shown that Murmur3 proved most accurate function staying within the relative error margin for all english words.

\subsection{Implementation}\label{algo-data}
The pseudo-code in the next section (see \ref{misra-pseudo}) describes our implementation of the \textit{Misra-Gries} algorithm combined with \textit{KMV}.  

\subsubsection{Misra-Gries and KMV design}\label{misra-gries}
The algorithm first initializes a dictionary of size \textit{k}. The keys in the dictionary correspond to topics \textit{j} encountered in the stream, and the values correspond to the frequency \text{f} of the topic. 

The OnTweet function is executed each time we see a new topic. If the topic is already in the dictionary, it's frequency will be increased by 1, otherwise the topic is added to dictionary with $f_{j}=1$ if the size of $A$ is less than $k$. If the size of $A$ is equal to $k$, all frequencies are decreased by 1, and removed if equal to 0 \cite{Amit}. Finally we return the key value pairs with the highest frequencies.
\\
To count the number of distinct values, we initialize a set $KMV_{j}$ for each topic, and aggregate the computed hash value each time we increment the frequency. The function \textit{KMV-Distinct} first sorts the \textit{KMV} set and returns a distinct value count based on the average space between the first \textit{k} values (see \ref{algo-distinct}).
\\ 
\begin{algorithm}
\caption{Misra-Gries Algorithm}\label{misra-pseudo}
\begin{algorithmic}[1]
\State $A\gets$ Initialize map
\State $KMV\gets$ Initialize $KMV_{j}$ set for topic $j$
\State $h\gets$ Hash function maps to $[0..1]$
\Statex
\Function{OnTweet}{$j$, $m$}
\If {$j \in keys(A)$}\Comment{Topic already seen}
    \State $A[j] \leftarrow A[j] + 1$
    \State $KMV_{j}$ add $h(m)$\Comment{Add hash value to set}
\ElsIf {$|keys(A)| < k-1 $}
    \State $A[j] \leftarrow 1$
    \State $KMV_{j}$ add $h(m)$
\Else
    \For{$l \in keys(A)$}
        \State {$A[l] \leftarrow A[l] - 1$}
        \If {$A[l] = 0$} 
        \State remove $A[l]$
        \State remove $KMV_{l}$
        \EndIf
    \EndFor
\EndIf
\EndFunction
\Statex
\Function{KMV-Distinct}{$k$, $j$}
\State $S\gets$ sorted KMV set
\State $distinct = k-1/S[k]$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Data structure}
When a new tweet is seen the algorithm has to determine if the contained topic has been seen before. We use a dictionary with key-value pair as topic-frequency, implemented internally in python as a hashtable. The dictionary provides $O(1)$ time lookup and increment operations. When we decrement all elements by one we iterate over the dictionary which takes $O(k)$ time, with $k$ being the maximum number of buckets in the dictionary. However, as we would need to touch all objects regardless of data structure, the efficiency of this operation is as good as can be for any data structure we have considered.

For each trending topic we create a set, which is also implemented internally in python as a hashtable. Each time a trending topic appears, we add the hash value of the tweet text to the set for calculating the K-min value. Like the dictionary, this provides us with a $O(1)$ insertion time. If a topic leaves the trending topics list, we also remove the accompanying set. This means that a topic that has been out of the trending topics list, will not have a history of distinct tweets if it returns to the list once again. However, our criteria for inclusion in the trending topics and the behavior of trending topics (most are distributed during the entire period) means this will rarely be the case, but we acknowledge that this will further deviate the distinct count from the true value.

For data retrieval, we have to do a sorting of both the trending topics dictionary, as we are only interested in the top 10 trending topics, as well as sorting the KMV set for each trending topic. We are aware that these data structures are optimized for random access and sorting them is a O(n log n) operation, but as result retrieval should be a rare occurance in comparison to the general running of the algorithms, we have optmized for the latter.

\subsubsection{Analysis of algorithm}
\label{algo-analysis}
Misra-Gries uses one pass with worst case running time $O(k)$ for each element of the stream $[n]$. Insertion and increments of a topic takes $O(1)$ time if the length of the dictionary is less than $k$. However the worst case scenario\footnote{Worst case: Every topic in the stream is unique, causing constant refreshing of the key-value dictionary} would decrement all values of the dictionary every time the dictionary is full, a maximum of $\frac{k}{n}$ times, so we argue that the amortized running time for each process is $O(1)$.

For our K-min value calculations, we do constant operations in calculating the hash value and inserting it. At no point do we touch more than one value.

Overall, this means that our algorithm for tracking both trending topics and associated distinct tweets runs in $O(n)$ time for the entire stream, exactly as expected for a streaming algorithm.

