\section{Streaming algorithms}\label{related work}
\subsection{Introduction}\label{algo-intro}
\begin{quote}\textit{
Twitter and other social networks are structured to accomodate personal communication across large networks of friends, and a such produce enormous amounts of data. The open availability of this data through developer APIs makes it's an interesting source for useful real-time information extraction using streaming algorithms \cite{genderprediction}. 
}
\end{quote}

This section introduces the concepts of algorithms that can compute some function of a massively long input stream $\sigma$ such as all public available tweets. In our model this is formalized as a sequence $\sigma = \langle a_{1}, a_{2},...,a_{m}\rangle $, where the elements of the sequence (in our case $tweets$) are drawn from the universe $[n] =: \{1, 2,..., n\}$. Note the two size parameters: the stream length, $m$, and the universe size, $n$.

\subsection{Goal}\label{algo-goals}
Our goal is to find the trending topics on twitter over a given time period. The idea is that our algorithm will be optimized for different intervals, eg. 1-hour and 24-hour periods, and that at any given time it can be queried to get the actual trending topics for previous interval (assuming the stream has been running for that minimum amount of time).
Our goal will be to process the input stream using a small amount of space $s$, i.e., to use s bits of random-access working memory. Since $m$ and $n$ are to be thought of ``huge'' we want to make $s$ much smaller than these. Ideally we want to achieve $s$ = O(log $m$ + log $n$), because this is the amount of space needed to store a constant number of elements from the stream and a constant number of counters that can count up to the length of the stream. \cite{Amit}

Specifically we want to analyze the Twitter input stream to find tags that that have a high occurence over some period of time, and be able to return these e.g. as a top-10 list for both 1 hour and 24 hour periods. Besides returning the most popular topics, we wish to be able to determine if a topic has a high frequency because many different people have mentioned it, or because it has been retweeted a lot of times.

\input{frequentitems.tex}

\subsection{Finding distinct values}\label{algo-distinct}
K-minimum values (KMV) is is a probalistic distinct value counting algorithm, that is intuitive and easy to implement \cite{kmv}. Suppose we have a good hash function that return evenly distributed values in the hash space $[0-1]$, then you could estimate the number of distinct values you have seen by knowing the average spacing between values in the hash space. The main challenge is to find a good hash function, and to select the number of minimum $k$ values on which to approximate the average spacing. If the hash values were indeed evenly distributed, we could keep just keep track of the minimum value, a get a good estimate of distinct values. However taking only one value opens up to a lot of variance and would rely heavily upon the``goodness'' of the hash function. In order to improve this Bar-Yossef\cite{Bar-Yossef} suggests keeping the k-smallest values, to give a more realistic estimate.

\subsubsection{Choice of hash function}\label{hash-function}
As mentioned in section \ref{algo-distinct} it's important to choose a hash function with an evenly dsitributed output to get a good estimate of distinct items with KMV. We are looking for a function that performs well for hashing text of lengths of approx. 3 to 140 characters. 

Cpesyna \cite{KMV1} performed an experiment with 4 different hash functions on different kinds of natural language inputs and varying sizes of $k$, among the input types were 99.171 english words, which we expect most of our data to be aswell. It was shown that Murmur3 proved most accurate function staying within the relative error margin for all english words.

\subsection{Implementation}\label{algo-data}
The pseudo-code in the next section (see \ref{misra-pseudo}) describes our implementation of the \textit{Misra-Gries} algorithm combined with \textit{KMV}.  

\subsubsection{Misra-Gries and KMV design}\label{misra-gries}
The algorithm first initializes a dictionary of size \textit{k}. The keys in the dictionary correspond to topics \textit{j} encountered in the stream, and the values correspond to the frequency \text{f} of the topic. 

The OnTweet function is executed each time we see a new topic. If the topic is already in the dictionary, it's frequency will be increased by 1, otherwise the topic is added to dictionary with $f_{j}=1$ if the size of $A$ is less than $k$. If the size of $A$ is equal to $k$, all frequencies are decreased by 1, and removed if equal to 0 \cite{Amit}. Finally we return the key value pairs with the highest frequencies.
\\
To count the number of distinct values, we initialize a set $KMV_{j}$ for each topic, and aggregate the computed hash value each time we increment the frequency. The function \textit{KMV-Distinct} first sorts the \textit{KMV} set and returns a distinct value count based on the average space between the first \textit{k} values (see \ref{algo-distinct}.
\\ 
\begin{algorithm}
\caption{Misra-Gries Algorithm}\label{misra-pseudo}
\begin{algorithmic}[1]
\State $A\gets$ Initialize map
\State $KMV\gets$ Initialize $KMV_{j}$ set for topic $j$
\State $h\gets$ Hash function maps to $[0..1]$
\Statex
\Function{OnTweet}{$j$, $m$}
\If {$j \in keys(A)$}\Comment{Topic already seen}
    \State $A[j] \leftarrow A[j] + 1$
    \State $KMV_{j}$ add $h(m)$\Comment{Add hash value to set}
\ElsIf {$|keys(A)| < k-1 $}
    \State $A[j] \leftarrow 1$
    \State $KMV_{j}$ add $h(m)$
\Else
    \For{$l \in keys(A)$}
        \State {$A[l] \leftarrow A[l] - 1$}
        \If {$A[l] = 0$} 
        \State remove $A[l]$
        \State remove $KMV_{l}$
        \EndIf
    \EndFor
\EndIf
\EndFunction
\Statex
\Function{KMV-Distinct}{$k$, $j$}
\State $S\gets$ sorted KMV set
\State $distinct = k-1/S[k]$
\EndFunction
\end{algorithmic}
\end{algorithm}

\subsubsection{Data structure}
When a new tweet is seen the algorithm has to determine if the contained topic has been seen before. We use a dictionary with key-value pair as topic-frequency. The dictionary provides $O(1)$ time lookup and increment operations. When we decrement all elements by one we iterate over the dictionary which takes $O(k)$ time. Finally we sort the dictionary, but since it's limited to $k$ items, this is not a very time consuming operation.

\subsubsection{Analysis of algorithm}\label{algo-analysis}
Misra-Gries uses one pass with worst case running time $O(k)$ for each element of the stream $[n]$. Insertion and increments of a topic takes $O(1)$ time if the length of the dictionary is less than $k$. However the worst case scenario would decrement all values of the dictionary every time the dictionary is full, a maximum of $\frac{k}{n}$ times, so we can argue that the amortized running time for each process is $O(1)$.


