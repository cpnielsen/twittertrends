\section{Streaming algorithms}\label{related work}
\subsection{Introduction}\label{algo-intro}
\begin{quote}\textit{
Twitter and other social networks are structured to accomodate personal communication across large networks of friends, and a such produce enormous amounts of data. The open availability of this data through developer APIs makes it's an interesting source for useful real-time information extraction using streaming algorithms \cite{genderprediction}. 
}
\end{quote}

This section describes briefly our goals for the project and how we, both theoretically and practically, make use of streaming algorithms to achieve these goals. We go through the process of finding suitable variables for our algorithms and using appropriate data structures.

\subsection{Goal}\label{algo-goals}
Our goal is to find the trending topics on twitter over a given time period. The idea is that our algorithm will be optimized for different intervals, 1-hour and 24-hour periods, and that at any given time it can be queried to get the actual topics trending. Additionally, we wish to find, for these topics, how many distinct tweets are posted to see if popular topics are the result of aggressive re-tweeting or actual interest in the topic as a whole.

To achieve this, we need to make use of streaming, lossy algorithms, for two major reasons:
\begin{enumerate}
    \item \textbf{To minimize our memory usage and allow for shorter processing time}
        It would be impractical, if not impossible, to keep the data for all tweets in-memory for a 24-hour period. In addition, processing 500.000 tweets in one go would take a significant amount of time - with streaming algorithms we can do simple calculations per tweet and spread the load.

    \item \textbf{To keep a running total of the results that we can query at any time}
        With an algorithm that works on a known set of data, we need to first collect that set of data and then post-process it. Additionally, we need to assign chunks of data to be processed at known intervals. Streaming algorithms allow us to get results at any time we desire, although the program have to have been running for the specified interval to get accurate results, based on our selection of constants.
\end{enumerate}

\input{frequentitems.tex}

\subsection{Implementation of algorithms}

\subsubsection{Data set}\label{algo-data}

\subsubsection{Misra-Gries Algorithm}\label{misra-gries}
The algorithm first initializes a dictionary with $k$ number of values. The keys in the dictionary are elements seen in the stream, and the value are counters associated with the elements. Then there is a process function that is executed each time we see a new element. If a new element is already in the dictionary, it's value will be increased by 1, otherwise if the number of elements in $A$ is less than $k$, the element will be inserted an its value set to 1. If the length of $A$ is equal to $k$, all values are decreased by 1, and removed if the value is equal to 0 \cite{Amit}. Finally we return the key value pairs with the highest frequencies.

We use Misra-Gries to find frequent $topics$ in our data stream of tweets using a one-pass algorithm, see pseudo-code \ref{misra-pseudo} for details.
\begin{algorithm}
\caption{Misra-Gries Algorithm}\label{misra-pseudo}
\begin{algorithmic}[1]
\State $A\gets$ Initialize map
\State $KMV\gets$ Initialize $KMV_{j}$ set for topic $j$
\State $h\gets$ Hash function maps to $[0..1]$
\Statex
\Function{OnTweet}{$j$, $m$}
\If {$j \in keys(A)$}\Comment{Topic already seen}
    \State $A[j] \leftarrow A[j] + 1$
    \State $KMV_{j}$ add $h(m)$\Comment{Add hash value to set}
\ElsIf {$|keys(A)| < k-1 $}
    \State $A[j] \leftarrow 1$
    \State $KMV_{j}$ add $h(m)$
\Else
    \For{$l \in keys(A)$}
        \State {$A[l] \leftarrow A[l] - 1$}
        \If {$A[l] = 0$} 
        \State remove $A[l]$
        \State remove $KMV_{l}$
        \EndIf
    \EndFor
\EndIf
\EndFunction
\Statex
\Function{KMV-Distinct}{$k$, $j$}
\State $S\gets$ sorted KMV set
\State $distinct = k-1/S[k]$
\EndFunction
\end{algorithmic}
\end{algorithm}
\subsubsection{Data structure}
When a new tweet is seen the algorithm has to determine if the contained topic has been seen before. We use a dictionary with key-value pair as topic-frequency. The dictionary provides $O(1)$ time lookup and increment operations. When we decrement all elements by one we iterate over the dictionary which takes $O(k)$ time. Finally we sort the dictionary, but since it's limited to $k$ items, this is not a very time consuming operation.

\subsubsection{Analysis of algorithm}\label{algo-analysis}
Misra-Gries uses one pass with worst case running time $O(k)$ for each element of the stream $[n]$. Insertion and increments of a topic takes $O(1)$ time if the length of the dictionary is less than $k$. However the worst case scenario would decrement all values of the dictionary every time the dictionary is full, a maximum of $\frac{k}{n}$ times, so we can argue that the amortized running time for each process is $O(1)$.

\subsection{Finding distinct values}\label{algo-distinct}
K-minimum values (KMV) is is a probalistic distinct value counting algorithm, that is intuitive and easy to implement \cite{kmv}. Suppose we have a good hash function that return evenly distributed values in the hash space $[0-1]$, then you could estimate the number of distinct values you have seen by knowing the average spacing between values in the hash space. The main challenge is to find a good hash function, and to select the number of minimum $k$ values on which to approximate the average spacing. If the hash values were indeed evenly distributed, we could keep just keep track of the minimum value, a get a good estimate of distinct values. However taking only one value opens up to a lot of variance and would rely heavily upon the``goodness'' of the hash function. In order to improve this Bar-Yossef\cite{Bar-Yossef} suggests keeping the k-smallest values, to give a more realistic estimate.

\subsubsection{Choice of hash function}\label{hash-function}
As mentioned in section \ref{algo-distinct} it's important to choose a hash function with an evenly dsitributed output to get a good estimate of distinct items with KMV. We are looking for a function that performs well for hashing text of lengths of approx. 3 to 140 characters. 

Cpesyna \cite{KMV1} performed an experiment with 4 different hash functions on different kinds of natural language inputs and varying sizes of $k$, among the input types were 99.171 english words, which we expect most of our data to be aswell. It was shown that Murmur3 proved most accurate function staying within the relative error margin for all english words.
