\subsection{Finding frequent items}\label{algo-frequent}
Ideally, we want to find the frequency of topics in $tweets$ in our input stream $\sigma = \langle a_{1}, a_{2},...,a_{m}\rangle$. We define a frequency vector $f = (f_{1},...,f_{n})$, where $f_{1} + ... + f_{n} = m$. 


To find the exact frequency of each topic would require that we store the exact number of times it has been seen, and with the vast number of topics seen (see section \ref{frequent-determine}) it would require a large amount of space to store. Instead, we can find the most frequent items seen over a period of time using the \textit{Misra-Gries} algorithm.\cite{Amit}
\newline

From the algorithm description we have $f_j - \frac{m}{k} \leq \hat{f_j} \leq f_j$. From this we can derive that any item with a $\hat{f_j} > \frac{m}{k}$ will be in our map $A[j]$ with a positive score, and we simply need to make sure the topics we wish to capture for a given time period are above the $\frac{m}{k}$ threshold. We will discover $m$ and derive $k$ from it in the following section.

\subsubsection{Determine k and m for Misra-Gries}
\label{frequent-determine}
In order for us to determine a suitable $k$ we will first have to find $m$ for a given timeframe. We wrote a piece of code that for 1 hour and 24 hours would count accurately \textit{a)} The total number of tweets, \textit{b)} The number of distinct topics and \textit{c)} The number of occurrances for each topic. We ran this a few times on different days to get an average on the number of tweet and occurances of the top 10 topics. An example of each sampling (1 hour and 24 hour respectively) can be seen below.\newline

Total number of tweets in 60 minutes: 26733, with 19117 unique topics\\

$\begin{array}{ll}
    454: & \#PeoplesChoice \\
    446: & \#StarAc \\
    331: & \#IHaveACrushOn \\
    316: & \#tbt \\
    265: & \#gameinsight \\
    251: & \#RT \\
    240: & \#TeamFollowBack \\
    223: & \#LouisAndBoris \\
    146: & \#musicfans \\
    140: & \#FF \\
\end{array}$
\\
\\

Total number of tweets in 1440 minutes: 534408, with 214345 unique topics\\

$\begin{array}{ll}
    17064: & \#FF \\
    7271: & \#gameinsight \\
    6589: & \#PeoplesChoice \\
    4775: & \#TeamFollowBack \\
    4433: & \#RT \\
    3692: & \#androidgames \\
    3522: & \#TFBJP \\
    3510: & \#MessageToMyEx \\
    3456: & \#ff \\
    3370: & \#android \\
\end{array}$
\\
\\
Given this data, we can determine an approximation for $k$. We want to include at least the top 10 topics for the time periods, so we maintain two seperate data structures for each. For the 60 minute period, we want topics with a minimum frequency of \textbf{140} occurrences over \textbf{26733} total events. This means we have $k = \frac{26733}{140} \approx 190$. If we take the average of all our samplings, we find the average to be $k = 155$ for a 24-hour period, and $k = 165$ for a 1-hour period. We assumed that $k$ would be larger for the 24 hour period, but as some topics keep repeating frequently spread over a day, they actually require a smaller k for bigger intervals. Also, our samples are not for longer periods (months), so this might not be completely accurate.
\\
\\
We discuss the results and our conclusions in section \ref{conclusions}
